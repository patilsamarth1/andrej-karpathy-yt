{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "words = open('../files/names.txt','r').read().splitlines()\n",
    "chars = sorted(list(set([c for w in words for c in w]))+['.'])\n",
    "\n",
    "ctoi = {c:i for i,c in enumerate(chars)}\n",
    "stoi = {s:i for i,s in enumerate(f'{c1}{c2}' for c1 in chars for c2 in chars)}\n",
    "\n",
    "itoc = {i:c for c,i in ctoi.items()}\n",
    "itos = {i:s for s,i in stoi.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E01** : train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model? \\\n",
    "\n",
    "Obvious improvement over the bigram model. Again, counts model (true probs) is better than NN. Tried to implement a decaying gradient coefficient to get closer to true minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Counts***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncounts = {}\\nfor w in words:\\n    w = '.'+w+'.'\\n    for c1, c2, c3 in zip(w, w[1:], w[2:]):\\n        trigram = (f'{c1}{c2}', c3)\\n        counts[trigram]= counts.get(trigram, 0) + 1\\n\\nstrs, chars = set([k[0] for k in counts.keys()]), sorted(list(set([k[1] for k in counts.keys()])))\\nlen(strs), len(chars)\\n\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0620)\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "import torch \n",
    "\n",
    "\n",
    "def train_counts_model(words):\n",
    "    # counts isn't really required\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    for w in words:\n",
    "        w = '.'+w+'.'\n",
    "        for c1, c2, c3 in zip(w, w[1:], w[2:]):\n",
    "            trigram = (f'{c1}{c2}', c3)\n",
    "            counts[trigram]= counts.get(trigram, 0) + 1\n",
    "\n",
    "    strs, chars = set([k[0] for k in counts.keys()]), sorted(list(set([k[1] for k in counts.keys()])))\n",
    "    len(strs), len(chars)\n",
    "    \"\"\"\n",
    "\n",
    "    N = torch.zeros((27*27, 27), dtype=torch.int32) # . doesn't follow .\n",
    "\n",
    "    for w in words:\n",
    "        w = '.'+w+'.'\n",
    "        for c1, c2, c3 in zip(w, w[1:], w[2:]):\n",
    "            s = f'{c1}{c2}'\n",
    "            c = c3\n",
    "            N[stoi[s], ctoi[c]] += 1\n",
    "        \n",
    "    P = (N).float()\n",
    "    P/=P.sum(1, keepdim=True)\n",
    "\n",
    "    # Loss\n",
    "    log_likelihood = 0.0\n",
    "\n",
    "    n=0\n",
    "    for w in words:\n",
    "        w = '.'+w+'.'\n",
    "        for c1, c2, c3 in zip(w, w[1:], w[2:]):\n",
    "            s = f'{c1}{c2}'\n",
    "            c = c3\n",
    "            log_likelihood-=torch.log(P[stoi[s], ctoi[c]])\n",
    "            n+=1\n",
    "\n",
    "    print(log_likelihood/n)\n",
    "\n",
    "train_counts_model(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0773, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2200, -0.9023, -1.0935,  ..., -0.4423,  0.0375, -0.3824],\n",
       "        [-4.0203,  1.4028,  1.3171,  ..., -0.6353,  1.2233,  1.0939],\n",
       "        [-1.2030,  4.7914, -1.1684,  ..., -1.3940,  0.8092, -1.2066],\n",
       "        ...,\n",
       "        [ 0.5510, -0.1960,  0.0327,  ..., -1.5166, -0.0539,  1.6817],\n",
       "        [ 3.3111,  3.0777, -1.8135,  ..., -1.1489, -0.9561, -0.5417],\n",
       "        [ 1.7711,  3.0932, -0.2070,  ..., -0.9813,  2.4313, -1.6206]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "def train_nn_model(words):\n",
    "    x,y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        w = '.'+w+'.'\n",
    "        for c1, c2, c3 in zip(w, w[1:], w[2:]):\n",
    "            x.append(stoi[f'{c1}{c2}'])\n",
    "            y.append(ctoi[c3])\n",
    "\n",
    "    x = torch.tensor(x)\n",
    "    y = torch.tensor(y)\n",
    "    xenc = torch.nn.functional.one_hot(x, num_classes=27*27).float()\n",
    "\n",
    "    W = torch.randn((27*27, 27), requires_grad=True)\n",
    "    ix = torch.arange(x.nelement())\n",
    "\n",
    "    # gradient descent\n",
    "    grad_coeff = 1000\n",
    "    losses = [1e10]\n",
    "    for k in range(1000):\n",
    "        outs = xenc@W\n",
    "        exp = outs.exp()\n",
    "        prob = exp/exp.sum(1, keepdim=True)\n",
    "        loss = -prob[ix, y].log().mean()\n",
    "        \n",
    "        if loss>losses[-1]:\n",
    "            grad_coeff/=2\n",
    "        losses.append(loss)\n",
    "        \n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        W.data -= 200*W.grad\n",
    "\n",
    "    print(min(losses))\n",
    "    \n",
    "    return W\n",
    "\n",
    "train_nn_model(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19700, 729])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_loss(x, y, W):\n",
    "    ix = torch.arange(x.nelement())\n",
    "    x_enc = torch.nn.functional.one_hot(dev_x, num_classes = 27*27).float()\n",
    "    exp = (x_enc@W).exp()\n",
    "    prob = exp/exp.sum(1, keepdim=True)\n",
    "    return -prob[ix, y].log().mean().shape\n",
    "\n",
    "get_loss(dev_x, dev_y, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**E02**: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0735, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train, dev, test = torch.utils.data.random_split(words, [0.8, 0.1, 0.1])\n",
    "\n",
    "W = train_nn_model(train)\n",
    "\n",
    "def get_loss(x, y, W):\n",
    "    ix = torch.arange(x.nelement())\n",
    "    x_enc = torch.nn.functional.one_hot(x, num_classes = 27*27).float()\n",
    "    exp = (x_enc@W).exp()\n",
    "    prob = exp/exp.sum(1, keepdim=True)\n",
    "    return -prob[ix, y].log().mean()\n",
    "\n",
    "def get_x_y(words):\n",
    "    x,y = [], []\n",
    "    for w in words:\n",
    "        w = '.'+w+'.'\n",
    "        for c1, c2, c3 in zip(w, w[1:], w[2:]):\n",
    "            x.append(stoi[f'{c1}{c2}'])\n",
    "            y.append(ctoi[c3])\n",
    "    x = torch.tensor(x)\n",
    "    y = torch.tensor(y)\n",
    "    return x,y\n",
    "\n",
    "dev_x, dev_y = get_x_y(dev)\n",
    "test_x, test_y = get_x_y(test)\n",
    "\n",
    "dev_loss = get_loss(dev_x, dev_y, W)\n",
    "test_loss = get_loss(test_x, test_y, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.1105, grad_fn=<NegBackward0>),\n",
       " tensor(2.1097, grad_fn=<NegBackward0>))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_loss, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "both dev and test losses are higher than training loss -> overfitting, high variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**E03**: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smooth strength: 0.009999999776482582, train loss: 2.092672109603882, dev loss: 2.11802077293396\n",
      "smooth strength: 0.019999999552965164, train loss: 2.107276439666748, dev loss: 2.118516206741333\n",
      "smooth strength: 0.029999999329447746, train loss: 2.1193363666534424, dev loss: 2.1201188564300537\n",
      "smooth strength: 0.03999999910593033, train loss: 2.1294806003570557, dev loss: 2.1220180988311768\n",
      "smooth strength: 0.05000000074505806, train loss: 2.1388325691223145, dev loss: 2.124297618865967\n",
      "smooth strength: 0.05999999865889549, train loss: 2.147226095199585, dev loss: 2.1268131732940674\n",
      "smooth strength: 0.07000000029802322, train loss: 2.155240774154663, dev loss: 2.1292643547058105\n",
      "smooth strength: 0.07999999821186066, train loss: 2.16278076171875, dev loss: 2.131679058074951\n",
      "smooth strength: 0.09000000357627869, train loss: 2.1700093746185303, dev loss: 2.134233236312866\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "def get_loss(x, y, W):\n",
    "    ix = torch.arange(x.nelement())\n",
    "    x_enc = torch.nn.functional.one_hot(x, num_classes = 27*27).float()\n",
    "    exp = (x_enc@W).exp()\n",
    "    prob = exp/exp.sum(1, keepdim=True)\n",
    "    return -prob[ix, y].log().mean()\n",
    "\n",
    "def get_x_y(words):\n",
    "    x,y = [], []\n",
    "    for w in words:\n",
    "        w = '.'+w+'.'\n",
    "        for c1, c2, c3 in zip(w, w[1:], w[2:]):\n",
    "            x.append(stoi[f'{c1}{c2}'])\n",
    "            y.append(ctoi[c3])\n",
    "    x = torch.tensor(x)\n",
    "    y = torch.tensor(y)\n",
    "    return x,y\n",
    "\n",
    "train, dev, test = torch.utils.data.random_split(words, [0.8, 0.1, 0.1])\n",
    "\n",
    "train_x, train_y = get_x_y(train)\n",
    "dev_x, dev_y = get_x_y(dev)\n",
    "\n",
    "for smooth_strength in torch.arange(0.01, 0.1, 0.01):\n",
    "    W = torch.randn((27*27, 27), requires_grad=True)\n",
    "\n",
    "    # gradient descent\n",
    "    grad_coeff = 1000\n",
    "    losses = [1e10]\n",
    "    for k in range(1000):\n",
    "        loss = get_loss(train_x, train_y, W)\n",
    "        # regularization\n",
    "        loss += smooth_strength * (W**2).mean()\n",
    "        if loss>losses[-1]:\n",
    "            grad_coeff/=2\n",
    "        losses.append(loss)\n",
    "        \n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        W.data -= 200*W.grad\n",
    "\n",
    "    print(f\"smooth strength: {smooth_strength}, train loss: {min(losses)}, dev loss: {get_loss(dev_x, dev_y, W)}\")\n",
    "\n",
    "\n",
    "test_x, test_y = get_x_y(test)\n",
    "\n",
    "test_loss = get_loss(test_x, test_y, W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "**E04**: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_enc = torch.nn.functional.one_hot(x, num_classes = 27*27).float()\n",
    "exp = (x_enc@W).exp()\n",
    "exp_new = W[x].exp()\n",
    "torch.equal(exp, exp_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**E05**: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simpler and more efficient\n",
    "- It actually handles the overflow issue internally by subracting the max logit value before applying softmax\n",
    "For ex. \n",
    "if model outputs are z=[1000,900,800], the exponentiations are too large \\\n",
    "we deal with this by normalizing the logits by subtracting the maximum logit value before applying softmax \\\n",
    "this works because shifting all logits by a constant does not change the softmax output (itâ€™s invariant to constant shifts). \\\n",
    "no all our logits are <=0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**E06**: meta-exercise! Think of a fun/interesting exercise and complete it.\n",
    "1. Does replacing . with different start and end tags change the loss? Why? \n",
    "1. What if we had different num of weights instead of (27, 27)\n",
    "1. Can the NN loss be lower than the counts model on the training set? What does it mean? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
